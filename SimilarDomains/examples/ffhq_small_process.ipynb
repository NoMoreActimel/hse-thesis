{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e69256c-ca74-44f4-abb7-7aeb5297f551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sasedov/StyleDomain/SimilarDomains\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasedov/.conda/envs/StyleDomain-env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/sasedov/StyleDomain/SimilarDomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e4a8d0-1c5a-4af3-bfe5-80374de64614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 bilinear 5 18 False False [4, 5, 6] (2, 2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import random\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from omegaconf import OmegaConf\n",
    "from core.utils.common import load_clip, mixing_noise\n",
    "from core.utils.example_utils import (\n",
    "    Inferencer, to_im, vstack_with_lines, hstack_with_lines, insert_image,\n",
    "    project_e4e, project_restyle_psp, project_fse_without_image_generation, read_img\n",
    ")\n",
    "from core.utils.image_utils import construct_paper_image_grid\n",
    "from core.utils.reading_weights import read_weights\n",
    "from core.uda_models import OffsetsTunningGenerator\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from examples.draw_util import weights, set_seed, morph_g_ema, IdentityEditor, StyleEditor\n",
    "\n",
    "from core.utils.example_utils import read_fse_config\n",
    "scale, scale_mode, idx_k, n_styles, enc_residual, enc_residual_coeff, resnet_layers, stride = read_fse_config('001')\n",
    "print(scale, scale_mode, idx_k, n_styles, enc_residual, enc_residual_coeff, resnet_layers, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24b89fd-5584-4e2e-b596-63286028cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_rows(rows, skip_horiz=10, skip_vertical=15):\n",
    "    final_image = [\n",
    "        hstack_with_lines(row_stack, skip_horiz) for row_stack in rows\n",
    "    ]\n",
    "\n",
    "    final_image = vstack_with_lines(final_image, skip_vertical)\n",
    "    return PIL.Image.fromarray(final_image)\n",
    "\n",
    "\n",
    "style_to_editor = {\n",
    "    d: StyleEditor(read_weights(weights[d])) for d in weights if d not in ['horse', 'car', 'ffhq', 'cat', 'church', 'to_metfaces', 'to_afhqcat', 'to_afhqdog', 'to_mega']\n",
    "}\n",
    "\n",
    "style_to_editor['original'] = IdentityEditor()\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d1589-506d-4bef-abd2-891a8f95aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFHQ-SMALL DATASET\n",
    "\n",
    "from test_dataset import SimpleImageDataset, align_image_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_path = '/home/sasedov/StyleDomain/faces_dataset_small/'\n",
    "# aligned_dataset_path = '/home/sasedov/StyleDomain/aligned_faces_dataset_small/'\n",
    "\n",
    "e4e_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "fse_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((1024, 1024)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# align_image_dataset(dataset_path, aligned_dataset_path)\n",
    "e4e_ffhq_small_dataset = SimpleImageDataset(dataset_path, e4e_transform)\n",
    "fse_ffhq_small_dataset = SimpleImageDataset(dataset_path, fse_transform)\n",
    "\n",
    "e4e_ffhq_small_dataloader = DataLoader(e4e_ffhq_small_dataset, batch_size=32, shuffle=False)\n",
    "fse_ffhq_small_dataloader = DataLoader(fse_ffhq_small_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4db8e0-5749-43ec-9bf4-c23b9cb96583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IMAGES, GET W-S AND FEATURE-MAPS\n",
    "\n",
    "# encoders' latents for ffhq small\n",
    "# do we want to compare generator feature maps with FSE feature maps too ?\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from tqdm import tqdm\n",
    "\n",
    "import io\n",
    "import os\n",
    "\n",
    "ws_e4e = []\n",
    "ws_fse = []\n",
    "fse_features_list = []\n",
    "orig_features_list = []\n",
    "filenames = []\n",
    "\n",
    "f_stdout = io.StringIO()\n",
    "\n",
    "gan_domain = 'ffhq'\n",
    "s_domain = 'jojo'\n",
    "\n",
    "ckpt = read_weights(weights[s_domain])\n",
    "ckpt_ffhq = {'sg2_params': ckpt['sg2_params']}\n",
    "ckpt_ffhq['sg2_params']['checkpoint_path'] = weights[gan_domain]\n",
    "\n",
    "model = Inferencer(ckpt, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with redirect_stdout(f_stdout):\n",
    "        for e4e_batch, fse_batch in tqdm(zip(e4e_ffhq_small_dataloader, fse_ffhq_small_dataloader), total=len(fse_ffhq_small_dataloader)):\n",
    "            \n",
    "            im_e4e_batch, names_e4e_batch = e4e_batch[\"image\"], e4e_batch[\"filename\"]\n",
    "            im_fse_batch, names_fse_batch = fse_batch[\"image\"], fse_batch[\"filename\"]\n",
    "            \n",
    "            assert names_e4e_batch == names_fse_batch, f'{names_e4e_batch}, {names_fse_batch}' \n",
    "            print(names_e4e_batch)\n",
    "            for name in names_e4e_batch:\n",
    "                filenames.append(name)\n",
    "            \n",
    "            w_e4e_batch = project_e4e(\n",
    "                im_e4e_batch,\n",
    "                'pretrained/e4e_ffhq_encode.pt',\n",
    "                use_transforms=False\n",
    "            )[1]\n",
    "\n",
    "            empty_img, w_fse_batch, fse_features_batch = project_fse_without_image_generation(\n",
    "                im_fse_batch,\n",
    "                model_path='pretrained/143_enc.pth',\n",
    "                fse_config_name='001',\n",
    "                arcface_model_path='pretrained/backbone.pth',\n",
    "                stylegan_model_path='pretrained/StyleGAN2/stylegan2-ffhq-config-f.pt',\n",
    "                use_transforms=False\n",
    "            )\n",
    "\n",
    "            features_batch = []\n",
    "            \n",
    "            for w_fse in w_fse_batch:\n",
    "                print(\"fse_latents.shape:\", w_fse.unsqueeze(0).shape)\n",
    "                kwargs = {\n",
    "                    \"latents\": [w_fse.unsqueeze(0)],\n",
    "                    \"input_is_latent\": True,\n",
    "                    \"features_in\": None,\n",
    "                    \"shift_with_generator_feature_map\": False,\n",
    "                    \"return_features\": True,\n",
    "                    \"return_generator_features\": True\n",
    "                }\n",
    "                imgs, features = model(**kwargs)\n",
    "                print(len(imgs), len(features))\n",
    "                print(len(features[0]), len(features[1]))\n",
    "                \n",
    "                print(features[0][5].shape, features[1][5].shape)\n",
    "                print(fse_features_batch[0][5].shape)\n",
    "                features_batch.append((features[0][5].detach(), features[1][5].detach()))\n",
    "            \n",
    "            print(f\"Orig features' norm: {features_batch[0][0].norm().detach().cpu().numpy()}\")\n",
    "            print(f\"Domain features' norm: {features_batch[0][1].norm().detach().cpu().numpy()}\")\n",
    "            print(f\"Norm of domain difference: {(features_batch[0][1] - features_batch[0][0]).norm().detach().cpu().numpy()}\")\n",
    "            \n",
    "            print(f\"FSE features' norm: {fse_features_batch[0][5].norm().detach().cpu().numpy()}\")\n",
    "            print(f\"Norm of FSE difference: {(fse_features_batch[0][5] - features_batch[0][0]).norm().detach().cpu().numpy()}\")\n",
    "            print(f\"Norm of FSE difference with domain shift: {(fse_features_batch[0][5] - features_batch[0][1]).norm().detach().cpu().numpy()}\")\n",
    "\n",
    "            for w_e4e, w_fse, fse_features, orig_features in zip(w_e4e_batch, w_fse_batch, fse_features_batch, features_batch):\n",
    "                ws_e4e.append(w_e4e)\n",
    "                ws_fse.append(w_fse)\n",
    "                fse_features_list.append(fse_features)\n",
    "                orig_features_list.append(orig_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b311880a-ce7e-437f-9654-bce1fc2ba810",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (w_e4e, w_fse) in enumerate(zip(ws_e4e, ws_fse)):\n",
    "    ws_e4e[i] = w_e4e.unsqueeze(0)\n",
    "    ws_fse[i] = w_fse.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaed976-2516-44d8-9121-01be262f25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws_e4e = torch.cat(ws_e4e, dim=0)\n",
    "# ws_fse = torch.cat(ws_fse, dim=0)\n",
    "\n",
    "fse_features_tensor = []\n",
    "skipped_indices = []\n",
    "for i, fse_features in enumerate(fse_features_list):\n",
    "    if fse_features is None:\n",
    "        print(i, ws_e4e[i].shape, ws_fse[i].shape)\n",
    "        skipped_indices.append(i)\n",
    "    else:\n",
    "        fse_features_tensor.append(fse_features[5])\n",
    "\n",
    "fse_features_tensor = torch.cat(fse_features_tensor, dim=0)\n",
    "\n",
    "\n",
    "orig_features_tensor_src = []\n",
    "orig_features_tensor_trg = []\n",
    "\n",
    "for i, orig_features in enumerate(orig_features_list):\n",
    "    orig_features_tensor_src.append(orig_features[0])\n",
    "    orig_features_tensor_trg.append(orig_features[1])\n",
    "\n",
    "orig_features_tensor_src = torch.cat(orig_features_tensor_src, dim=0)\n",
    "orig_features_tensor_trg = torch.cat(orig_features_tensor_trg, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2557be-df94-4bb7-92b2-6879266c081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsfsfs \n",
    "torch.save(ws_e4e, f\"ws_e4e_tensor.pt\")\n",
    "torch.save(ws_fse, f\"ws_fse_tensor.pt\")\n",
    "torch.save(fse_features_tensor, f\"fse_features_tesnor.pt\")\n",
    "torch.save(orig_features_tensor_src, f\"orig_features_tensor_src.pt\")\n",
    "torch.save(orig_features_tensor_trg, f\"orig_features_tensor_trg.pt\")\n",
    "\n",
    "with open('latent_corr_exp_filenames.txt', 'w') as f:\n",
    "    for name in filenames:\n",
    "        f.write(f\"{name}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-StyleDomain-env]",
   "language": "python",
   "name": "conda-env-.conda-StyleDomain-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
