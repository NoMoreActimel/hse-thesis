{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83738daf-7c25-4dbd-b4ab-1fe030cd7b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sasedov/StyleDomain/SimilarDomains\n"
     ]
    }
   ],
   "source": [
    "%cd /home/sasedov/StyleDomain/SimilarDomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5422a3c0-fc22-42f2-950e-efabc9508a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasedov/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 bilinear 5 18 False False [4, 5, 6] (2, 2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import random\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from omegaconf import OmegaConf\n",
    "from core.utils.common import load_clip, mixing_noise\n",
    "from core.utils.example_utils import (\n",
    "    Inferencer, to_im, vstack_with_lines, hstack_with_lines, insert_image,\n",
    "    project_e4e, project_restyle_psp, project_fse_without_image_generation, read_img\n",
    ")\n",
    "from core.utils.image_utils import construct_paper_image_grid\n",
    "from core.utils.reading_weights import read_weights\n",
    "from core.uda_models import OffsetsTunningGenerator\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from examples.draw_util import weights, set_seed, morph_g_ema, IdentityEditor, StyleEditor\n",
    "\n",
    "from core.utils.example_utils import read_fse_config\n",
    "\n",
    "scale, scale_mode, idx_k, n_styles, enc_residual, enc_residual_coeff, resnet_layers, stride = read_fse_config('FeatureStyleEncoder/configs/001.yaml')\n",
    "print(scale, scale_mode, idx_k, n_styles, enc_residual, enc_residual_coeff, resnet_layers, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15843fc7-fd4b-4c0a-9ed5-ecb6d7ed24dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/sasedov/StyleDomain/SimilarDomains/pretrained/StyleGAN2/stylegan2-ffhq-config-f.pt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['ffhq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "514b71b2-8f39-4258-98cd-5f1c44a22512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_rows(rows, skip_horiz=10, skip_vertical=15):\n",
    "    final_image = [\n",
    "        hstack_with_lines(row_stack, skip_horiz) for row_stack in rows\n",
    "    ]\n",
    "\n",
    "    final_image = vstack_with_lines(final_image, skip_vertical)\n",
    "    return PIL.Image.fromarray(final_image)\n",
    "\n",
    "\n",
    "style_to_editor = {\n",
    "    d: StyleEditor(read_weights(weights[d])) for d in weights if d not in ['horse', 'car', 'ffhq', 'cat', 'church', 'to_metfaces', 'to_afhqcat', 'to_afhqdog', 'to_mega']\n",
    "}\n",
    "\n",
    "style_to_editor['original'] = IdentityEditor()\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "202612a7-6a7c-4d8f-a4e8-cb06571b4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "from PIL import ImageFont\n",
    "from visualize import stack_to_grid_with_names, text_on_square_image\n",
    "import io\n",
    "import sys\n",
    "\n",
    "def test_s_domains_on_different_encoders(\n",
    "        gan_domain, s_domains, domain_ims_or_texts, im,\n",
    "        w_enc_list, features_list, generator_shift_list,\n",
    "        image_size=256, verbose=False,\n",
    "        row_names=None, column_names=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "        domain_ims_or_texts: list of initial images or text descriptions per each domain, empty images by default\n",
    "        w_enc_list: list of latents for each encoder\n",
    "        features_list: list of encoder feature maps for each encoder, should be None by default\n",
    "        generator_shift_list: list of flags, whether to shift with generator feature maps\n",
    "    \"\"\"\n",
    "    \n",
    "    if domain_ims_or_texts is None:\n",
    "        domain_ims_or_texts = [PIL.Image.fromarray(np.zeros_like(im))] * len(s_domains)\n",
    "    \n",
    "    resize = Resize(image_size)\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((1024, 1024)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    def prepare_im(im, use_transform=False, unsqueeze=False):\n",
    "        if use_transform: \n",
    "            im = transform(im)\n",
    "        if unsqueeze:\n",
    "            im = im.unsqueeze(0)\n",
    "        return np.array(to_im(resize(im), padding=0))\n",
    "    \n",
    "    row_stack = []\n",
    "    row = []\n",
    "    inversion_row = []\n",
    "    \n",
    "    assert len(w_enc_list) == len(features_list) and len(w_enc_list) == len(generator_shift_list)\n",
    "    images_per_row = len(w_enc_list) + 1\n",
    "\n",
    "    if not verbose:\n",
    "        f_stdout = io.StringIO()\n",
    "\n",
    "    for i, (s_domain, domain_info) in enumerate(zip(s_domains, domain_ims_or_texts)):\n",
    "        ckpt = read_weights(weights[s_domain])\n",
    "        ckpt_ffhq = {'sg2_params': ckpt['sg2_params']}\n",
    "        ckpt_ffhq['sg2_params']['checkpoint_path'] = weights[gan_domain]\n",
    "\n",
    "        if not verbose:\n",
    "            with redirect_stdout(f_stdout):\n",
    "                model = Inferencer(ckpt, device)\n",
    "        \n",
    "        if i == 0:\n",
    "            inversion_row.append(prepare_im(im, use_transform=True, unsqueeze=True))\n",
    "        \n",
    "        if isinstance(domain_info, str):\n",
    "            row.append(text_on_square_image(\n",
    "                domain_info, image_size,\n",
    "                font=ImageFont.truetype(\"/home/sasedov/Times.ttf\", 25 * image_size // 256),\n",
    "                linewidth=20\n",
    "            ))\n",
    "        else:\n",
    "            row.append(prepare_im(domain_info, use_transform=True, unsqueeze=True))\n",
    "        \n",
    "        for j, (w_enc, features, generator_shift) in enumerate(zip(w_enc_list, features_list, generator_shift_list)):\n",
    "            kwargs = {\n",
    "                \"latents\": [w_enc],\n",
    "                \"input_is_latent\": True,\n",
    "                \"features_in\": features,\n",
    "                \"shift_with_generator_feature_map\": generator_shift,\n",
    "                \"offsets_coeffs\": None #offsets_coeffs_list[j] if offsets_coeffs_list is not None else None\n",
    "            }\n",
    "            if not verbose:\n",
    "                with redirect_stdout(f_stdout):\n",
    "                    src, trg = model(**kwargs)\n",
    "            else:\n",
    "                src, trg = model(**kwargs)\n",
    "                \n",
    "            # add original image and its inversion to row_stack\n",
    "            if i == 0: \n",
    "                inversion_row.append(prepare_im(src))\n",
    "            row.append(prepare_im(trg))\n",
    "\n",
    "        # if i == 0:\n",
    "        #     row_stack.append(inversion_row)\n",
    "        row_stack.append(row)\n",
    "        row = []\n",
    "\n",
    "    # return stack_rows(row_stack)\n",
    "\n",
    "    return stack_to_grid_with_names(\n",
    "        imgs_list=row_stack, H=image_size, W=image_size,\n",
    "        row_names=row_names, column_names=column_names,\n",
    "        font=ImageFont.truetype(\"/home/sasedov/Times.ttf\", 25 * image_size // 256),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40379778-5dca-4d8f-b16c-31a74d2409c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from contextlib import redirect_stdout\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "def get_image_domains():\n",
    "    f_stdout = io.StringIO()\n",
    "\n",
    "    domain_images_path = 'image_domains/'\n",
    "    domain_images = {}\n",
    "\n",
    "    for domain_image_filename in tqdm(os.listdir(domain_images_path)):\n",
    "        if '.' not in domain_image_filename:\n",
    "            continue\n",
    "        s_domain, file_ext = domain_image_filename.split('.')\n",
    "        if file_ext in ['png', 'jpg'] and s_domain != 'anime' and s_domain in weights:\n",
    "            with redirect_stdout(f_stdout):\n",
    "                domain_images[s_domain] = read_img(domain_images_path + domain_image_filename, align_input=False)\n",
    "\n",
    "    domain_images = domain_images.items()\n",
    "    s_domains = [x[0] for x in domain_images]\n",
    "    domain_ims = [x[1] for x in domain_images]\n",
    "\n",
    "    ignored_image_domains = ['joker', 'sketch']\n",
    "    for domain in ignored_image_domains:\n",
    "        if domain in s_domains:\n",
    "            ind = s_domains.index(domain)\n",
    "            s_domains.pop(ind)\n",
    "            domain_ims.pop(ind)\n",
    "\n",
    "    print('Considering the following image domains:', *s_domains)\n",
    "    return s_domains, domain_ims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "715d5210-c275-4f94-a7e5-882e41e8797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def get_text_domains(domain_dir_path='pretrained/checkpoints_iccv'):\n",
    "    path = Path(domain_dir_path)\n",
    "    \n",
    "    s_domains = []\n",
    "    domain_texts = []\n",
    "    \n",
    "    for domain_dir in tqdm(path.iterdir()):\n",
    "        with open(domain_dir / 'config.yaml', 'r') as file:\n",
    "            domain_config = yaml.safe_load(file)\n",
    "        \n",
    "        if domain_config['training']['target_class'][-4:] not in ['.png', '.jpg']:\n",
    "            s_domains.append(domain_dir.name[:-7])  # cut out _sdelta in the end\n",
    "            domain_texts.append(domain_config['training']['target_class'])\n",
    "            if 'indomain' in s_domains[-1]:\n",
    "                domain_texts[-1] += ' (indomain)'\n",
    "\n",
    "    print('Considering the following text domains:', *s_domains)\n",
    "    return s_domains, domain_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8510dfd9-7c94-4320-9e2d-185de8d88b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:00<00:00, 771.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering the following image domains: jojo nigelwy_untitled_artwork_18 digital_painting_jing titan_armin oliver_wetter_corie_lynn_concept_1_final_back_web murasaki_nora_asuya disney_princess remi_castaneda_sean_connery_final stanislav_galai_04_23 anastasia maria_trepalina_img_6833_1 rain_artwork_dtiyschallennge mermaid truc_huynh_hex_009v2 speed_paint naufal_ilyasa_seraphine_ig_full_res rich_d_amaru_studyfinal ricardo_viana_render2 wesley_gardner_portraitstudy titan_erwin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "im_domains, domain_ims = get_image_domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "165095b7-0564-472c-9205-c55af44fc674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [00:02, 32.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering the following text domains: edvard_munch_painting the_thanos tolkien_elf_indomain neanderthal_indomain modigliani_painting sketch hulk minimalist_drawing zombie_indomain pop_art_indomain pixar hulk_indomain edvard_munch_painting_indomain frida_kahlo_painting hermione_granger claude_monet_painting dali_painting neanderthal ukiyo-e botero_indomain frida_kahlo_painting_indomain impressionist_drawing_indomain werewolf_indomain impressionism_painting_indomain dali_painting_indomain botero modigliani_painting_indomain constructivism_indomain impressionist_drawing constructivism minimalist_drawing_indomain joker pop_art anime_indomain anime tolkien_elf werewolf ukiyo-e_indomain zombie impressionism_painting disney_princess_indomain disney_princess hermione_granger_indomain cubism_painting_indomain sketch_indomain claude_monet_painting_indomain cubism_painting the_thanos_indomain pixar_indomain joker_indomain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_domains, domain_texts = get_text_domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "698db17d-1d69-4d50-b507-2f000459b07f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s_domains = im_domains + text_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36fd0e30-7af0-4c6b-b21c-147b5f719557",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_ims_or_texts = domain_ims + domain_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeba82f4-afff-4932-acaa-af8d4e66d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEASURE LAYER OFFSETS FOR EACH DOMAIN\n",
    "def measure_offset_norms_by_layer(s_domains):\n",
    "    gan_domain = 'ffhq'\n",
    "    # s_domains.append('pop_art_indomain')\n",
    "\n",
    "    layer_offsets_by_domain = {}\n",
    "    mean_layer_norms = defaultdict(float)\n",
    "\n",
    "    for s_domain in s_domains:\n",
    "        print(f'\\nDOMAIN {s_domain}:')\n",
    "        ckpt = read_weights(weights[s_domain])\n",
    "        ckpt_ffhq = {'sg2_params': ckpt['sg2_params']}\n",
    "        ckpt_ffhq['sg2_params']['checkpoint_path'] = weights[gan_domain]\n",
    "\n",
    "        model = Inferencer(ckpt, device)\n",
    "\n",
    "        layer_offsets_by_domain[s_domain] = {}\n",
    "        for layer_name, layer_value in model.model_da().items():\n",
    "            layer_norm = layer_value['in'].norm().detach().cpu().numpy()\n",
    "            print(f'{layer_name}: {layer_norm}, layer_value.shape: {layer_value[\"in\"].shape}')\n",
    "\n",
    "            mean_layer_norms[layer_name] += layer_norm\n",
    "            layer_offsets_by_domain[s_domain][layer_name] = layer_value['in'].detach()\n",
    "\n",
    "    print(f'\\nMean layer norms across selected s_domains:')\n",
    "    for layer_name, layer_norm in mean_layer_norms.items():\n",
    "        mean_layer_norms[layer_name] = layer_norm / len(mean_layer_norms)\n",
    "        print(f'{layer_name}: {mean_layer_norms[layer_name]}')\n",
    "    \n",
    "    return mean_layer_norms, layer_offsets_by_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcf8f8e3-6e31-42a0-9498-0220164515e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_layer_norms, layer_offsets_by_domain = measure_offset_norms_by_layer(im_domains + text_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00ed468c-1ab0-4d86-89ef-069addb51ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "e4e_offsets_coeffs = [\n",
    "    0.7250403761863708,\n",
    "    0.2626339495182037,\n",
    "    0.6552985906600952,\n",
    "    0.7303584218025208,\n",
    "    0.6811474561691284,\n",
    "    0.7767353653907776,\n",
    "    0.9527875781059265,\n",
    "    0.9049936532974243,\n",
    "    0.8513546586036682,\n",
    "    0.7799299359321594,\n",
    "    1.1332719326019287,\n",
    "    0.7100310325622559,\n",
    "    0.7962964177131653,\n",
    "    0.6757728457450867,\n",
    "    0.7018265724182129,\n",
    "    0.6636937856674194,\n",
    "    0.6807562708854675,\n",
    "    0.6779585480690002\n",
    "]\n",
    "\n",
    "fse_offsets_coeffs = [\n",
    "    0.46660858392715454,\n",
    "    0.00757311424240469,\n",
    "    0.4669175148010254,\n",
    "    0.7105019092559814,\n",
    "    0.9221417307853699,\n",
    "    0.6862319111824036,\n",
    "    1.1542936563491821,\n",
    "    0.8920652866363525,\n",
    "    1.0488144159317017,\n",
    "    0.5729503631591797,\n",
    "    1.4492567777633667,\n",
    "    0.6482011675834656,\n",
    "    1.731235384941101,\n",
    "    0.5105834603309631,\n",
    "    1.0992273092269897,\n",
    "    0.5512086153030396,\n",
    "    0.8076189160346985,\n",
    "    0.6648847460746765\n",
    "]\n",
    "e4e_offsets_coeffs = [1 / x for x in e4e_offsets_coeffs]\n",
    "fse_offsets_coeffs = [1 / x for x in fse_offsets_coeffs]\n",
    "\n",
    "offsets_coeffs_list = [\n",
    "    None, e4e_offsets_coeffs,\n",
    "    None, fse_offsets_coeffs,\n",
    "    None, fse_offsets_coeffs,\n",
    "    None, fse_offsets_coeffs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75a65e83-0a39-48d2-a9e5-7ff99b4ac87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [\n",
    "    i for i, s_domain in enumerate(im_domains)\n",
    "    if s_domain in [\"murasaki_nora_asuya\", \"rich_d_amaru_studyfinal\"]\n",
    "    # if s_domain in [\"oliver_wetter_corie_lynn_concept_1_final_back_web\", \"titan_armin\"]\n",
    "]\n",
    "\n",
    "tmp_im_domains = [im_domains[i] for i in indices]\n",
    "tmp_domain_ims = [domain_ims[i] for i in indices]\n",
    "\n",
    "indices = [\n",
    "    i for i, s_domain in enumerate(text_domains)\n",
    "    # if s_domain in [\"murasaki_nora_asuya\"]\n",
    "    if s_domain in [\"hulk\", \"frida_kahlo_painting\", \"modigliani_painting\", \"modigliani_painting_indomain\"]\n",
    "]\n",
    "indices.insert(1, indices[-1])\n",
    "indices.pop()\n",
    "\n",
    "tmp_text_domains = [text_domains[i] for i in indices]\n",
    "tmp_domain_texts = [domain_texts[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9815b701-8631-411d-b6bf-4de7d16e0ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned image has shape: (1024, 1024)\n",
      "Loading e4e over the pSp framework from checkpoint: pretrained/e4e_ffhq_encode.pt\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([3, 1024, 1024])\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "List of used encoders:\n",
      "1) E4E latents\n",
      "2) FSE with latents only\n",
      "3) FSE with latents and feature map\n",
      "4) FSE with latents, feature map and generator shift\n",
      "225 24\n",
      "225 24\n",
      "127 25\n",
      "57 19\n",
      "240 24\n"
     ]
    }
   ],
   "source": [
    "# TESTING OFFSETS COEFFS\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "gan_domain = 'ffhq'\n",
    "\n",
    "# image_path = '/home/sasedov/misha.jpg'\n",
    "image_path = 'examples/photos/01703.png'\n",
    "im = read_img(image_path, align_input=True)\n",
    "\n",
    "w_e4e = project_e4e(im, 'pretrained/e4e_ffhq_encode.pt')[1]\n",
    "empty_img, w_fse, fse_features = project_fse_without_image_generation(\n",
    "    im,\n",
    "    model_path='pretrained/143_enc.pth',\n",
    "    fse_config_name='001',\n",
    "    arcface_model_path='pretrained/backbone.pth',\n",
    "    stylegan_model_path='pretrained/StyleGAN2/stylegan2-ffhq-config-f.pt'\n",
    ")\n",
    "\n",
    "print('List of used encoders:')\n",
    "print('1) E4E latents')\n",
    "print('2) FSE with latents only')\n",
    "print('3) FSE with latents and feature map')\n",
    "print('4) FSE with latents, feature map and generator shift')\n",
    "\n",
    "result = test_s_domains_on_different_encoders(\n",
    "    gan_domain,\n",
    "    s_domains=tmp_im_domains + tmp_text_domains,\n",
    "    domain_ims_or_texts=tmp_domain_ims + tmp_domain_texts,\n",
    "    im=im,\n",
    "    w_enc_list=[w_e4e, w_e4e, w_fse, w_fse, w_fse, w_fse, w_fse, w_fse],\n",
    "    features_list=[None, None, None, None, fse_features, fse_features, fse_features, fse_features], \n",
    "    generator_shift_list=[False, False, False, False, False, False, True, True],\n",
    "    image_size=256, verbose=False,\n",
    "    column_names=[\n",
    "        \"orig\",\n",
    "        \"e4e latents\", \"e4e latents coeffs\",\n",
    "        \"fse latents\", \"fse latents coeffs\",\n",
    "        \"fse features\", \"fse features coeffs\",\n",
    "        \"fse gen shift\", \"fse gen shift coeffs\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "name='1703_selected'\n",
    "result.save(f'encoders_comparison_results/offsets_coeffs/{name}_encoders_comparison_on_s_domains.pdf', save_all=True, append_images=[])\n",
    "result.save(f'encoders_comparison_results/offsets_coeffs/{name}_encoders_comparison_on_s_domains.jpg')\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebe3b34a-6738-47c5-be45-8478ae0310b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [\n",
    "    i for i, s_domain in enumerate(im_domains)\n",
    "    if s_domain in []\n",
    "    # if s_domain in [\"oliver_wetter_corie_lynn_concept_1_final_back_web\", \"titan_armin\"]\n",
    "]\n",
    "\n",
    "tmp_im_domains = [im_domains[i] for i in indices]\n",
    "tmp_domain_ims = [domain_ims[i] for i in indices]\n",
    "\n",
    "indices = [\n",
    "    i for i, s_domain in enumerate(text_domains)\n",
    "    # if s_domain in [\"murasaki_nora_asuya\"]\n",
    "    if s_domain in [\"frida_kahlo_painting\", \"hermione_granger\", \"claude_monet_painting\", \"dali_painting\"]\n",
    "]\n",
    "\n",
    "tmp_text_domains = [text_domains[i] for i in indices]\n",
    "tmp_domain_texts = [domain_texts[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b351731-9d6e-4a0b-acb6-718d33488019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned image has shape: (1024, 1024)\n",
      "Loading e4e over the pSp framework from checkpoint: pretrained/e4e_ffhq_encode.pt\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([3, 1024, 1024])\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "List of used encoders:\n",
      "1) E4E latents\n",
      "2) FSE with latents only\n",
      "3) FSE with latents and feature map\n",
      "4) FSE with latents, feature map and generator shift\n",
      "961 95\n",
      "864 95\n",
      "652 97\n",
      "654 76\n",
      "375 95\n",
      "605 95\n"
     ]
    }
   ],
   "source": [
    "# HEADINGS FOR SCREENSHOTS\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "gan_domain = 'ffhq'\n",
    "\n",
    "image_path = 'examples/photos/01703.png'\n",
    "im = read_img(image_path, align_input=True)\n",
    "\n",
    "w_e4e = project_e4e(im, 'pretrained/e4e_ffhq_encode.pt')[1]\n",
    "empty_img, w_fse, fse_features = project_fse_without_image_generation(\n",
    "    im,\n",
    "    model_path='pretrained/143_enc.pth',\n",
    "    fse_config_name='001',\n",
    "    arcface_model_path='pretrained/backbone.pth',\n",
    "    stylegan_model_path='pretrained/StyleGAN2/stylegan2-ffhq-config-f.pt'\n",
    ")\n",
    "\n",
    "print('List of used encoders:')\n",
    "print('1) E4E latents')\n",
    "print('2) FSE with latents only')\n",
    "print('3) FSE with latents and feature map')\n",
    "print('4) FSE with latents, feature map and generator shift')\n",
    "\n",
    "result = test_s_domains_on_different_encoders(\n",
    "    gan_domain,\n",
    "    s_domains=tmp_text_domains,\n",
    "    domain_ims_or_texts=tmp_domain_texts,\n",
    "    im=im,\n",
    "    w_enc_list=[w_e4e, w_fse, w_fse, w_fse],\n",
    "    features_list=[None, None, fse_features, fse_features], \n",
    "    generator_shift_list=[False, False, False, True],\n",
    "    image_size=1024, verbose=False,\n",
    "    column_names=[\"orig\", \"e4e latents\", \"fse latents\", \"fse features\", \"fse generator shift\"]\n",
    ")\n",
    "\n",
    "name='1703_text_domains'\n",
    "result.save(f'encoders_comparison_results/tests/headings/{name}.pdf', save_all=True, append_images=[])\n",
    "result.save(f'encoders_comparison_results/tests/headings/{name}.jpg')\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b841a854-bb8f-428d-9aab-a289277033e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# image_path = '/home/sasedov/misha.jpg'\u001b[39;00m\n\u001b[1;32m      7\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexamples/photos/01703.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mread_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m w_e4e \u001b[38;5;241m=\u001b[39m project_e4e(im, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained/e4e_ffhq_encode.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     11\u001b[0m empty_img, w_fse, fse_features \u001b[38;5;241m=\u001b[39m project_fse_without_image_generation(\n\u001b[1;32m     12\u001b[0m     im,\n\u001b[1;32m     13\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained/143_enc.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     stylegan_model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained/StyleGAN2/stylegan2-ffhq-config-f.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/StyleDomain/SimilarDomains/core/utils/example_utils.py:29\u001b[0m, in \u001b[0;36mread_img\u001b[0;34m(img_path, align_input)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_img\u001b[39m(img_path, align_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m align_input:\n\u001b[0;32m---> 29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(img_path)\n",
      "File \u001b[0;32m~/StyleDomain/SimilarDomains/core/utils/example_utils.py:375\u001b[0m, in \u001b[0;36mrun_alignment\u001b[0;34m(image_path, predictor_path)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdlib shape predictor is not downloaded; launch `python download.py --load_type=dlib`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    374\u001b[0m predictor \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mshape_predictor(predictor_path)\n\u001b[0;32m--> 375\u001b[0m aligned_image \u001b[38;5;241m=\u001b[39m \u001b[43malign_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAligned image has shape: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(aligned_image\u001b[38;5;241m.\u001b[39msize))\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m aligned_image\n",
      "File \u001b[0;32m~/StyleDomain/SimilarDomains/core/utils/common.py:321\u001b[0m, in \u001b[0;36malign_face\u001b[0;34m(filepath, predictor, output_size, transform_size, enable_padding)\u001b[0m\n\u001b[1;32m    318\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mminimum(np\u001b[38;5;241m.\u001b[39mfloat32(x) \u001b[38;5;241m/\u001b[39m pad[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mfloat32(w \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m x) \u001b[38;5;241m/\u001b[39m pad[\u001b[38;5;241m2\u001b[39m]),\n\u001b[1;32m    319\u001b[0m                   \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mminimum(np\u001b[38;5;241m.\u001b[39mfloat32(y) \u001b[38;5;241m/\u001b[39m pad[\u001b[38;5;241m1\u001b[39m], np\u001b[38;5;241m.\u001b[39mfloat32(h \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m/\u001b[39m pad[\u001b[38;5;241m3\u001b[39m]))\n\u001b[1;32m    320\u001b[0m blur \u001b[38;5;241m=\u001b[39m qsize \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.02\u001b[39m\n\u001b[0;32m--> 321\u001b[0m img \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgaussian_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mblur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m img) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(mask \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    322\u001b[0m img \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mmedian(img, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m-\u001b[39m img) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(mask, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    323\u001b[0m img \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39muint8(np\u001b[38;5;241m.\u001b[39mclip(np\u001b[38;5;241m.\u001b[39mrint(img), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/StyleDomain-env/lib/python3.10/site-packages/scipy/ndimage/_filters.py:385\u001b[0m, in \u001b[0;36mgaussian_filter\u001b[0;34m(input, sigma, order, output, mode, cval, truncate, radius, axes)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(axes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, sigma, order, mode, radius \u001b[38;5;129;01min\u001b[39;00m axes:\n\u001b[0;32m--> 385\u001b[0m         \u001b[43mgaussian_filter1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/StyleDomain-env/lib/python3.10/site-packages/scipy/ndimage/_filters.py:283\u001b[0m, in \u001b[0;36mgaussian_filter1d\u001b[0;34m(input, sigma, axis, order, output, mode, cval, truncate, radius)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Since we are calling correlate, not convolve, revert the kernel\u001b[39;00m\n\u001b[1;32m    282\u001b[0m weights \u001b[38;5;241m=\u001b[39m _gaussian_kernel1d(sigma, order, lw)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorrelate1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/StyleDomain-env/lib/python3.10/site-packages/scipy/ndimage/_filters.py:140\u001b[0m, in \u001b[0;36mcorrelate1d\u001b[0;34m(input, weights, axis, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid origin; origin must satisfy \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    137\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-(len(weights) // 2) <= origin <= \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    138\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(len(weights)-1) // 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m mode \u001b[38;5;241m=\u001b[39m _ni_support\u001b[38;5;241m.\u001b[39m_extend_mode_to_code(mode)\n\u001b[0;32m--> 140\u001b[0m \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelate1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m                      \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "gan_domain = 'ffhq'\n",
    "\n",
    "# image_path = '/home/sasedov/misha.jpg'\n",
    "image_path = 'examples/photos/01703.png'\n",
    "im = read_img(image_path, align_input=True)\n",
    "\n",
    "w_e4e = project_e4e(im, 'pretrained/e4e_ffhq_encode.pt')[1]\n",
    "empty_img, w_fse, fse_features = project_fse_without_image_generation(\n",
    "    im,\n",
    "    model_path='pretrained/143_enc.pth',\n",
    "    fse_config_name='001',\n",
    "    arcface_model_path='pretrained/backbone.pth',\n",
    "    stylegan_model_path='pretrained/StyleGAN2/stylegan2-ffhq-config-f.pt'\n",
    ")\n",
    "\n",
    "print('List of used encoders:')\n",
    "print('1) E4E latents')\n",
    "print('2) FSE with latents only')\n",
    "print('3) FSE with latents and feature map')\n",
    "print('4) FSE with latents, feature map and generator shift')\n",
    "\n",
    "result = test_s_domains_on_different_encoders(\n",
    "    gan_domain,\n",
    "    s_domains=im_domains + text_domains,\n",
    "    domain_ims_or_texts=domain_ims + domain_texts,\n",
    "    im=im,\n",
    "    w_enc_list=[w_e4e, w_fse, w_fse, w_fse],\n",
    "    features_list=[None, None, fse_features, fse_features], \n",
    "    generator_shift_list=[False, False, False, True],\n",
    "    image_size=256, verbose=False,\n",
    "    column_names=[\"orig\", \"e4e latents\", \"fse latents\", \"fse features\", \"fse generator shift\"]\n",
    ")\n",
    "\n",
    "name='blue_headscarf_01703'\n",
    "result.save(f'encoders_comparison_results/tests/{name}_encoders_comparison_on_s_domains.pdf', save_all=True, append_images=[])\n",
    "result.save(f'encoders_comparison_results/tests/{name}_encoders_comparison_on_s_domains.jpg')\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7883acc5-281e-4869-9201-47060f680af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned image has shape: (1024, 1024)\n",
      "Loading e4e over the pSp framework from checkpoint: pretrained/e4e_ffhq_encode.pt\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([3, 1024, 1024])\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "List of used encoders:\n",
      "1) E4E latents\n",
      "2) FSE with latents only\n",
      "3) FSE with latents and feature map\n",
      "4) FSE with latents, feature map and generator shift\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "gan_domain = 'ffhq'\n",
    "\n",
    "image_path = '/home/sasedov/misha.jpg'\n",
    "im = read_img(image_path, align_input=True)\n",
    "\n",
    "w_e4e = project_e4e(im, 'pretrained/e4e_ffhq_encode.pt')[1]\n",
    "empty_img, w_fse, fse_features = project_fse_without_image_generation(\n",
    "    im,\n",
    "    model_path='pretrained/143_enc.pth',\n",
    "    fse_config_name='001',\n",
    "    arcface_model_path='pretrained/backbone.pth',\n",
    "    stylegan_model_path='pretrained/StyleGAN2/stylegan2-ffhq-config-f.pt'\n",
    ")\n",
    "\n",
    "print('List of used encoders:')\n",
    "print('1) E4E latents')\n",
    "print('2) FSE with latents only')\n",
    "print('3) FSE with latents and feature map')\n",
    "print('4) FSE with latents, feature map and generator shift')\n",
    "\n",
    "result = test_s_domains_on_different_encoders(\n",
    "    gan_domain,\n",
    "    s_domains=text_domains,\n",
    "    domain_ims_or_texts=domain_texts,\n",
    "    im=im,\n",
    "    w_enc_list=[w_e4e, w_fse, w_fse, w_fse],\n",
    "    features_list=[None, None, fse_features, fse_features], \n",
    "    generator_shift_list=[False, False, False, True],\n",
    "    image_size=1024, verbose=False,\n",
    "    column_names=[\"orig\", \"e4e latents\", \"fse latents\", \"fse features\", \"fse generator shift\"]\n",
    ")\n",
    "\n",
    "name='misha_text_domains'\n",
    "result.save(f'encoders_comparison_results/{name}_encoders_comparison_on_s_domains.pdf', save_all=True, append_images=[])\n",
    "result.save(f'encoders_comparison_results/{name}_encoders_comparison_on_s_domains.jpg')\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb71115-59b7-4b60-b81d-f4fa9da0a1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: pretrained/e4e_ffhq_encode.pt\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([3, 1024, 1024])\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "List of used encoders:\n",
      "1) E4E latents\n",
      "2) FSE with latents only\n",
      "3) FSE with latents and feature map\n",
      "4) FSE with latents, feature map and generator shift\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "gan_domain = 'ffhq'\n",
    "\n",
    "image_path = '/home/sasedov/luka2-square.jpg'\n",
    "im = read_img(image_path, align_input=False)\n",
    "\n",
    "w_e4e = project_e4e(im, 'pretrained/e4e_ffhq_encode.pt')[1]\n",
    "empty_img, w_fse, fse_features = project_fse_without_image_generation(\n",
    "    im,\n",
    "    model_path='pretrained/143_enc.pth',\n",
    "    fse_config_name='001',\n",
    "    arcface_model_path='pretrained/backbone.pth',\n",
    "    stylegan_model_path='pretrained/StyleGAN2/stylegan2-ffhq-config-f.pt'\n",
    ")\n",
    "\n",
    "print('List of used encoders:')\n",
    "print('1) E4E latents')\n",
    "print('2) FSE with latents only')\n",
    "print('3) FSE with latents and feature map')\n",
    "print('4) FSE with latents, feature map and generator shift')\n",
    "\n",
    "result = test_s_domains_on_different_encoders(\n",
    "    gan_domain,\n",
    "    s_domains=im_domains + text_domains,\n",
    "    domain_ims_or_texts=domain_ims + domain_texts,\n",
    "    im=im,\n",
    "    w_enc_list=[w_e4e, w_fse, w_fse, w_fse],\n",
    "    features_list=[None, None, fse_features, fse_features], \n",
    "    generator_shift_list=[False, False, False, True],\n",
    "    image_size=256, verbose=False,\n",
    "    column_names=[\"orig\", \"e4e latents\", \"fse latents\", \"fse features\", \"fse generator shift\"]\n",
    ")\n",
    "\n",
    "name='luka_all_domains'\n",
    "result.save(f'encoders_comparison_results/{name}_encoders_comparison_on_s_domains.pdf', save_all=True, append_images=[])\n",
    "result.save(f'encoders_comparison_results/{name}_encoders_comparison_on_s_domains.jpg')\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9eb5d-8a69-44eb-be6b-665658443b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889ba4d-0d4f-4613-9a56-269f3e4f7080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN COMPARISON ON THE CLIP-BASED QUALITY METRIC DIFF FOR FSE_WITH_FEATURES AND E4E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8efd9077-2035-4a38-8218-a0c91abe83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_types = {}\n",
    "\n",
    "for domain_type in ['all', 'image', 'text']:\n",
    "    domain_types[domain_type] = {}\n",
    "    for domain_quality_type in ['diff', 'mid', 'sim']:\n",
    "        type_filestr = '' if domain_type == 'all' else domain_type + '_'\n",
    "        \n",
    "        with open(f'domain_encoder_metrics/{type_filestr}{domain_quality_type}_domains_list.txt', 'r') as f:\n",
    "            domain_types[domain_type][domain_quality_type] = set()\n",
    "            \n",
    "            for line in f.readlines():\n",
    "                s_domain = line.strip().split(' ')[0]\n",
    "                \n",
    "                if domain_type == 'image' and s_domain in ignored_image_domains:\n",
    "                    continue\n",
    "                \n",
    "                domain_types[domain_type][domain_quality_type].add(s_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21d186a6-f3f4-49ef-84ad-7ef4f6c6ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_indices_by_type = {}\n",
    "s_domains_by_type = {}\n",
    "domain_ims_or_texts_by_type = {}\n",
    "\n",
    "for domain_type, domain_groups in domain_types.items():\n",
    "    domain_indices_by_type[domain_type] = {}\n",
    "    s_domains_by_type[domain_type] = {}\n",
    "    domain_ims_or_texts_by_type[domain_type] = {}\n",
    "    \n",
    "    for key, domains in domain_groups.items():\n",
    "        domain_indices_by_type[domain_type][key] = [i for i, d in enumerate(s_domains) if d in domains]\n",
    "        s_domains_by_type[domain_type][key] = [s_domains[i] for i in domain_indices_by_type[domain_type][key]]\n",
    "        domain_ims_or_texts_by_type[domain_type][key] = [domain_ims_or_texts[i] for i in domain_indices_by_type[domain_type][key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e902d06-95e8-47f9-8614-3abcfeb60888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5332b650-1c66-45a9-8b11-de6a2f3cfa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 3, 8, 12, 14, 16, 18, 19], 20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_indices_by_type['image']['diff'], len(im_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6040dc97-b473-40a7-8ed1-a71bbca230f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 34, 36, 37, 38, 45, 46, 55, 56, 62]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_indices_by_type['text']['diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d26011da-7a72-4a0f-af98-39a57c29fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: pretrained/e4e_ffhq_encode.pt\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([3, 1024, 1024])\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "List of used encoders:\n",
      "1) E4E latents\n",
      "2) FSE with latents only\n",
      "3) FSE with latents and feature map\n",
      "4) FSE with latents, feature map and generator shift\n",
      "Processing all domains of diff quality type...\n",
      "Saved results to encoders_comparison_results/domains_by_type_comparison/all/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains in .pdf and .jpg !\n",
      "Processing all domains of mid quality type...\n",
      "Saved results to encoders_comparison_results/domains_by_type_comparison/all/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains in .pdf and .jpg !\n",
      "Processing all domains of sim quality type...\n",
      "Saved results to encoders_comparison_results/domains_by_type_comparison/all/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains in .pdf and .jpg !\n",
      "Processing image domains of diff quality type...\n",
      "Saved results to encoders_comparison_results/domains_by_type_comparison/image/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains in .pdf and .jpg !\n",
      "Processing image domains of mid quality type...\n",
      "Saved results to encoders_comparison_results/domains_by_type_comparison/image/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains in .pdf and .jpg !\n",
      "Processing image domains of sim quality type...\n",
      "Saved results to encoders_comparison_results/domains_by_type_comparison/image/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains in .pdf and .jpg !\n",
      "Processing text domains of diff quality type...\n",
      "Saved results to encoders_comparison_results/domains_by_type_comparison/text/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains in .pdf and .jpg !\n",
      "Processing text domains of mid quality type...\n",
      "Saved results to encoders_comparison_results/domains_by_type_comparison/text/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains in .pdf and .jpg !\n",
      "Processing text domains of sim quality type...\n",
      "Saved results to encoders_comparison_results/domains_by_type_comparison/text/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains in .pdf and .jpg !\n"
     ]
    }
   ],
   "source": [
    "# BY DOMAIN_TYPE ['ALL', 'IMAGE', 'TEXT'] AND DOMAIN_QUALITY_TYPE IN ['DIFF', 'MID', 'SIM']\n",
    "\n",
    "NAME = 'BLUE_HEADSCARF_01703'\n",
    "WRITE_DIR_PATH = 'encoders_comparison_results/domains_by_type_comparison'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "gan_domain = 'ffhq'\n",
    "\n",
    "# image_path = '/home/sasedov/luka2-square.jpg'\n",
    "image_path = 'examples/photos/01703.png'\n",
    "im = read_img(image_path, align_input=False)\n",
    "\n",
    "w_e4e = project_e4e(im, 'pretrained/e4e_ffhq_encode.pt')[1]\n",
    "empty_img, w_fse, fse_features = project_fse_without_image_generation(\n",
    "    im,\n",
    "    model_path='pretrained/143_enc.pth',\n",
    "    fse_config_name='001',\n",
    "    arcface_model_path='pretrained/backbone.pth',\n",
    "    stylegan_model_path='pretrained/StyleGAN2/stylegan2-ffhq-config-f.pt'\n",
    ")\n",
    "\n",
    "print('List of used encoders:')\n",
    "print('1) E4E latents')\n",
    "print('2) FSE with latents only')\n",
    "print('3) FSE with latents and feature map')\n",
    "print('4) FSE with latents, feature map and generator shift')\n",
    "\n",
    "for DOMAIN_TYPE in ['all', 'image', 'text']:\n",
    "    for DOMAIN_QUALITY_TYPE in ['diff', 'mid', 'sim']:\n",
    "        print(f'Processing {DOMAIN_TYPE} domains of {DOMAIN_QUALITY_TYPE} quality type...')\n",
    "\n",
    "        result = test_s_domains_on_different_encoders(\n",
    "            gan_domain,\n",
    "            s_domains=s_domains_by_type[DOMAIN_TYPE][DOMAIN_QUALITY_TYPE],\n",
    "            domain_ims_or_texts=domain_ims_or_texts_by_type[DOMAIN_TYPE][DOMAIN_QUALITY_TYPE],\n",
    "            im=im,\n",
    "            w_enc_list=[w_e4e, w_fse, w_fse, w_fse],\n",
    "            features_list=[None, None, fse_features, fse_features], \n",
    "            generator_shift_list=[False, False, False, True],\n",
    "            image_size=256, verbose=False,\n",
    "            column_names=[\"orig\", \"e4e latents\", \"fse latents\", \"fse features\", \"fse generator shift\"]\n",
    "        )\n",
    "\n",
    "        filepath = f'{WRITE_DIR_PATH}/{DOMAIN_TYPE}/{DOMAIN_QUALITY_TYPE}_domains_{NAME}_encoders_comparison_on_s_domains'\n",
    "        result.save(f'{filepath}.pdf', save_all=True, append_images=[])\n",
    "        result.save(f'{filepath}.jpg')\n",
    "        print(f'Saved results to {filepath} in .pdf and .jpg !')\n",
    "        # result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1d8d5ea-b1bc-4b37-b7fc-c22140491fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: encoders_comparison_results/domains_by_type_comparison/ (stored 0%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/sim_domains_luka_all_domains_encoders_comparison_on_s_domains.jpg (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 1%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/.ipynb_checkpoints/diff_domains_luka_all_domains_encoders_comparison_on_s_domains-checkpoint.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/.ipynb_checkpoints/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains-checkpoint.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/.ipynb_checkpoints/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains-checkpoint.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/.ipynb_checkpoints/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains-checkpoint.pdf (deflated 1%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/all/ (stored 0%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/all/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 1%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/all/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/all/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/all/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 1%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/all/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/all/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/diff_domains_luka_all_domains_encoders_comparison_on_s_domains.jpg (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 1%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/mid_domains_luka_all_domains_encoders_comparison_on_s_domains.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/sim_domains_luka_all_domains_encoders_comparison_on_s_domains.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/mid_domains_luka_all_domains_encoders_comparison_on_s_domains.jpg (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/text/ (stored 0%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/text/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 1%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/text/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/text/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/text/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 1%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/text/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/text/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/diff_domains_luka_all_domains_encoders_comparison_on_s_domains.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/image/ (stored 0%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/image/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 1%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/image/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 1%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/image/mid_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 1%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/image/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/image/.ipynb_checkpoints/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains-checkpoint.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/image/sim_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/image/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.jpg (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/image/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 2%)\n",
      "  adding: encoders_comparison_results/domains_by_type_comparison/diff_domains_BLUE_HEADSCARF_01703_encoders_comparison_on_s_domains.pdf (deflated 2%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r domains_by_type_comparison.zip encoders_comparison_results/domains_by_type_comparison/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1eb7276-89b6-4ac2-926e-9f7427e2a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: domain_encoder_metrics/ (stored 0%)\n",
      "  adding: domain_encoder_metrics/domain_encoder_metrics_.txt (deflated 86%)\n",
      "  adding: domain_encoder_metrics/mean_encoder_metrics_by_domains_2.csv (deflated 63%)\n",
      "  adding: domain_encoder_metrics/image_sim_domains_list.txt (deflated 27%)\n",
      "  adding: domain_encoder_metrics/text_sim_domains_list.txt (deflated 70%)\n",
      "  adding: domain_encoder_metrics/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: domain_encoder_metrics/.ipynb_checkpoints/domain_encoder_metrics_-checkpoint.txt (deflated 86%)\n",
      "  adding: domain_encoder_metrics/.ipynb_checkpoints/text_mid_domains_list-checkpoint.txt (deflated 67%)\n",
      "  adding: domain_encoder_metrics/.ipynb_checkpoints/text_domain_encoder_metrics-checkpoint.txt (deflated 88%)\n",
      "  adding: domain_encoder_metrics/.ipynb_checkpoints/text_sim_domains_list-checkpoint.txt (deflated 70%)\n",
      "  adding: domain_encoder_metrics/.ipynb_checkpoints/diff_domains_list-checkpoint.txt (stored 0%)\n",
      "  adding: domain_encoder_metrics/.ipynb_checkpoints/image_mid_domains_list-checkpoint.txt (deflated 42%)\n",
      "  adding: domain_encoder_metrics/.ipynb_checkpoints/text_diff_domains_list-checkpoint.txt (deflated 53%)\n",
      "  adding: domain_encoder_metrics/.ipynb_checkpoints/image_diff_domains_list-checkpoint.txt (deflated 43%)\n",
      "  adding: domain_encoder_metrics/.ipynb_checkpoints/image_sim_domains_list-checkpoint.txt (deflated 74%)\n",
      "  adding: domain_encoder_metrics/diff_domains_list.txt (deflated 48%)\n",
      "  adding: domain_encoder_metrics/image_domain_encoder_metrics.txt (deflated 85%)\n",
      "  adding: domain_encoder_metrics/image_diff_domains_list.txt (deflated 43%)\n",
      "  adding: domain_encoder_metrics/encoder_metrics_by_domains.csv (deflated 90%)\n",
      "  adding: domain_encoder_metrics/image_mid_domains_list.txt (deflated 39%)\n",
      "  adding: domain_encoder_metrics/text_mid_domains_list.txt (deflated 67%)\n",
      "  adding: domain_encoder_metrics/mid_domains_list.txt (deflated 62%)\n",
      "  adding: domain_encoder_metrics/encoder_metrics_by_domains_2.csv (deflated 90%)\n",
      "  adding: domain_encoder_metrics/text_diff_domains_list.txt (deflated 54%)\n",
      "  adding: domain_encoder_metrics/text_domain_encoder_metrics.txt (deflated 88%)\n",
      "  adding: domain_encoder_metrics/sim_domains_list.txt (deflated 72%)\n",
      "  adding: domain_encoder_metrics/mean_encoder_metrics_by_domains.csv (deflated 63%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r domain_encoder_metrics.zip domain_encoder_metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87b71a-fda1-48f9-bf06-699971aa7bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/job-1911463/ipykernel_268044/4180398877.py:10: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  width, height = font.getsize(line)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MOVED TO VISUALIZE.PY\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def text_on_square_image(text, image_size, font=ImageFont.truetype(\"/home/sasedov/Times.ttf\", 25)):\n",
    "    im = Image.new('RGB', (image_size, image_size), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    \n",
    "    y_text = image_size / 2\n",
    "    lines = textwrap.wrap(text, width=20)\n",
    "    for line in lines:\n",
    "        width, height = font.getsize(line)\n",
    "        draw.text(((image_size - width) / 2, y_text), line, font=font, fill='black')\n",
    "        y_text += height\n",
    "    \n",
    "    # draw.text(((image_size - w) / 2, (image_size - h) / 2), text, font=font, fill='black')\n",
    "    return np.array(im)\n",
    "    \n",
    "Image.fromarray(text_on_square_image(\"Edvard Munch Painting\", image_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5e80fc-869d-4548-bcfd-66532f722b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_e4e = torch.load(f\"ws_e4e_tensor.pt\", map_location='cuda:0')\n",
    "ws_fse = torch.load(f\"ws_fse_tensor.pt\", map_location='cuda:0')\n",
    "fse_features_tensor = torch.load(\"fse_features_tesnor.pt\", map_location=device)\n",
    "orig_features_tensor_src = torch.load(\"orig_features_tensor_src.pt\", map_location=device)\n",
    "orig_features_tensor_trg = torch.load(\"orig_features_tensor_trg.pt\", map_location=device)\n",
    "\n",
    "filenames = []\n",
    "with open('latent_corr_exp_filenames.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        filenames.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf1ac61-5168-4f3d-862e-9f98442f37a3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jojo png\n",
      "Aligned image has shape: (1024, 1024)\n",
      "nigelwy_untitled_artwork_18 jpg\n",
      "Aligned image has shape: (1024, 1024)\n",
      "digital_painting_jing png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_ext \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m s_domain \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manime\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m s_domain \u001b[38;5;129;01min\u001b[39;00m weights:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(s_domain, file_ext)\n\u001b[0;32m---> 24\u001b[0m         domain_images[s_domain] \u001b[38;5;241m=\u001b[39m \u001b[43mread_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdomain_images_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdomain_image_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m domain_images \u001b[38;5;241m=\u001b[39m domain_images\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     27\u001b[0m s_domains \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m domain_images]\n",
      "File \u001b[0;32m~/StyleDomain/SimilarDomains/core/utils/example_utils.py:29\u001b[0m, in \u001b[0;36mread_img\u001b[0;34m(img_path, align_input)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_img\u001b[39m(img_path, align_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m align_input:\n\u001b[0;32m---> 29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(img_path)\n",
      "File \u001b[0;32m~/StyleDomain/SimilarDomains/core/utils/example_utils.py:374\u001b[0m, in \u001b[0;36mrun_alignment\u001b[0;34m(image_path, predictor_path)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(predictor_path):\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdlib shape predictor is not downloaded; launch `python download.py --load_type=dlib`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 374\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m aligned_image \u001b[38;5;241m=\u001b[39m align_face(filepath\u001b[38;5;241m=\u001b[39mimage_path, predictor\u001b[38;5;241m=\u001b[39mpredictor) \n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAligned image has shape: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(aligned_image\u001b[38;5;241m.\u001b[39msize))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "gan_domain = 'ffhq'\n",
    "hard_image_indices = [316, 837, 704, 1985, 839]\n",
    "\n",
    "dataset_path = '/home/sasedov/StyleDomain/faces_dataset_small/'\n",
    "\n",
    "domain_images_path = 'image_domains/'\n",
    "domain_images = {}\n",
    "\n",
    "for domain_image_filename in os.listdir(domain_images_path):\n",
    "    s_domain, file_ext = domain_image_filename.split('.')\n",
    "    if file_ext in ['png', 'jpg'] and s_domain != 'anime' and s_domain in weights:\n",
    "        print(s_domain, file_ext)\n",
    "        domain_images[s_domain] = read_img(domain_images_path + domain_image_filename, align_input=True)\n",
    "\n",
    "domain_images = domain_images.items()\n",
    "s_domains = [x[0] for x in domain_images]\n",
    "domain_ims = [x[1] for x in domain_images]\n",
    "\n",
    "print('Considering the following domains:', *s_domains)\n",
    "\n",
    "for image_ind in hard_image_indices:\n",
    "\n",
    "    image_path = dataset_path + filenames[image_ind]\n",
    "    im = read_img(image_path, align_input=True)\n",
    "\n",
    "    w_e4e = project_e4e(im, 'pretrained/e4e_ffhq_encode.pt')[1]\n",
    "    empty_img, w_fse, fse_features = project_fse_without_image_generation(\n",
    "        im,\n",
    "        model_path='pretrained/143_enc.pth',\n",
    "        fse_config_name='001',\n",
    "        arcface_model_path='pretrained/backbone.pth',\n",
    "        stylegan_model_path='pretrained/StyleGAN2/stylegan2-ffhq-config-f.pt'\n",
    "    )\n",
    "\n",
    "    print('List of used encoders:')\n",
    "    print('1) E4E latents')\n",
    "    print('2) FSE with latents only')\n",
    "    print('3) FSE with latents and feature map')\n",
    "    print('4) FSE with latents, feature map and generator shift')\n",
    "\n",
    "    result = test_s_domains_on_different_encoders(\n",
    "        gan_domain, s_domains, domain_ims=domain_ims, im=im,\n",
    "        w_enc_list=[w_e4e, w_fse, w_fse, w_fse],\n",
    "        features_list=[None, None, fse_features, fse_features], \n",
    "        generator_shift_list=[False, False, False, True],\n",
    "        image_size=256, verbose=False\n",
    "    )\n",
    "    result.save(f'img_{image_ind}_encoders_comparison_on_s_domains.pdf', save_all=True, append_images=[])\n",
    "    result.save(f'img_{image_ind}_encoders_comparison_on_s_domains.jpg')\n",
    "    print(f'Saved results for {image_ind}!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-StyleDomain-env]",
   "language": "python",
   "name": "conda-env-.conda-StyleDomain-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
