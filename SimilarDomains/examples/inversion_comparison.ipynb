{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6217ee98-ff85-4515-8063-66a962245afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sasedov/StyleDomain/SimilarDomains\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasedov/.conda/envs/StyleDomain-env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/sasedov/StyleDomain/SimilarDomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dedd255-9d53-482b-a684-36b1a6ebbd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasedov/.conda/envs/StyleDomain-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 bilinear 5 18 False False [4, 5, 6] (2, 2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import random\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from omegaconf import OmegaConf\n",
    "from core.utils.common import load_clip, mixing_noise\n",
    "from core.utils.example_utils import (\n",
    "    Inferencer, to_im, vstack_with_lines, hstack_with_lines, insert_image,\n",
    "    project_e4e, project_restyle_psp, project_fse_without_image_generation, read_img\n",
    ")\n",
    "from core.utils.image_utils import construct_paper_image_grid\n",
    "from core.utils.reading_weights import read_weights\n",
    "from core.uda_models import OffsetsTunningGenerator\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from examples.draw_util import weights, set_seed, morph_g_ema, IdentityEditor, StyleEditor\n",
    "\n",
    "from core.utils.example_utils import read_fse_config\n",
    "scale, scale_mode, idx_k, n_styles, enc_residual, enc_residual_coeff, resnet_layers, stride = read_fse_config('001')\n",
    "print(scale, scale_mode, idx_k, n_styles, enc_residual, enc_residual_coeff, resnet_layers, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55290a4f-817a-4359-b8b9-96fa7f26f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_rows(rows, skip_horiz=10, skip_vertical=15):\n",
    "    final_image = [\n",
    "        hstack_with_lines(row_stack, skip_horiz) for row_stack in rows\n",
    "    ]\n",
    "\n",
    "    final_image = vstack_with_lines(final_image, skip_vertical)\n",
    "    return PIL.Image.fromarray(final_image)\n",
    "\n",
    "\n",
    "style_to_editor = {\n",
    "    d: StyleEditor(read_weights(weights[d])) for d in weights if d not in ['horse', 'car', 'ffhq', 'cat', 'church', 'to_metfaces', 'to_afhqcat', 'to_afhqdog', 'to_mega']\n",
    "}\n",
    "\n",
    "style_to_editor['original'] = IdentityEditor()\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e23b10f-c218-40d5-b66b-a95644977b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_e4e = torch.load(f\"ws_e4e_tensor.pt\", map_location='cuda:0')\n",
    "ws_fse = torch.load(f\"ws_fse_tensor.pt\", map_location='cuda:0')\n",
    "fse_features_tensor = torch.load(\"fse_features_tesnor.pt\", map_location=device)\n",
    "orig_features_tensor_src = torch.load(\"orig_features_tensor_src.pt\", map_location=device)\n",
    "orig_features_tensor_trg = torch.load(\"orig_features_tensor_trg.pt\", map_location=device)\n",
    "\n",
    "filenames = []\n",
    "with open('latent_corr_exp_filenames.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        filenames.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f75db26-8d73-42c4-a8a3-e4b35afbcbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "from visualize import stack_to_grid_with_names\n",
    "from PIL import ImageFont\n",
    "import io\n",
    "import sys\n",
    "\n",
    "def test_inversion_on_different_encoders(\n",
    "        gan_domain, s_domain, ims,\n",
    "        w_enc_list, features_list, generator_shift_list,\n",
    "        image_size=256, verbose=False,\n",
    "        row_names=None, column_names=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "        ims: list of initial images\n",
    "        w_enc_list: list of lists latents for each encoder\n",
    "        features_list: list of lists of encoder feature maps for each image and encoder, should be None by default\n",
    "        generator_shift_list: list of lists of flags, whether to shift with generator feature maps\n",
    "    \"\"\"\n",
    "    resize = Resize(image_size)\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((1024, 1024)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    def prepare_im(im, use_transform=False, unsqueeze=False):\n",
    "        if use_transform: \n",
    "            im = transform(im)\n",
    "        if unsqueeze:\n",
    "            im = im.unsqueeze(0)\n",
    "        return np.array(to_im(resize(im), padding=0))\n",
    "    \n",
    "    row_stack = []\n",
    "    row = []\n",
    "    \n",
    "    assert len(w_enc_list) == len(features_list) and len(w_enc_list) == len(generator_shift_list)\n",
    "    images_per_row = len(w_enc_list[0]) + 1\n",
    "\n",
    "    if not verbose:\n",
    "        f_stdout = io.StringIO()\n",
    "\n",
    "    for i, im in enumerate(ims):\n",
    "        ckpt = read_weights(weights[s_domain])\n",
    "        ckpt_ffhq = {'sg2_params': ckpt['sg2_params']}\n",
    "        ckpt_ffhq['sg2_params']['checkpoint_path'] = weights[gan_domain]\n",
    "\n",
    "        if not verbose:\n",
    "            with redirect_stdout(f_stdout):\n",
    "                model = Inferencer(ckpt, device)\n",
    "        \n",
    "        row.append(prepare_im(im, use_transform=True, unsqueeze=True))\n",
    "        \n",
    "        w_enc_im_list = w_enc_list[i]\n",
    "        features_im_list = features_list[i]\n",
    "        generator_shift_im_list = generator_shift_list[i]\n",
    "        \n",
    "        for j, (w_enc, features, generator_shift) in enumerate(zip(w_enc_im_list, features_im_list, generator_shift_im_list)):\n",
    "            kwargs = {\n",
    "                \"latents\": [w_enc],\n",
    "                \"input_is_latent\": True,\n",
    "                \"features_in\": features,\n",
    "                \"shift_with_generator_feature_map\": generator_shift\n",
    "            }\n",
    "            if not verbose:\n",
    "                with redirect_stdout(f_stdout):\n",
    "                    src, trg = model(**kwargs)\n",
    "            else:\n",
    "                src, trg = model(**kwargs)\n",
    "                \n",
    "            # add original image and its inversion to row_stack\n",
    "            row.append(prepare_im(src))\n",
    "\n",
    "        row_stack.append(row)\n",
    "        row = []\n",
    "\n",
    "    return stack_to_grid_with_names(\n",
    "        imgs_list=row_stack, H=image_size, W=image_size,\n",
    "        row_names=row_names, column_names=column_names,\n",
    "        font=ImageFont.truetype(\"/home/sasedov/Times.ttf\", 25 * image_size // 256),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6bda978-a484-4ddd-b2ac-fe9f3130f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "\n",
    "\n",
    "def inverse_images(indices=None, n_images=16, use_random_indices=True, dataset_path='/home/sasedov/StyleDomain/faces_dataset_small/'):\n",
    "    if use_random_indices:\n",
    "        indices = np.random.randint(0, len(filenames), n_images)\n",
    "    else:\n",
    "        n_images = len(indices)\n",
    "\n",
    "    ims = []\n",
    "    ws_ims = []\n",
    "    fse_features_ims = []\n",
    "    generator_shift_ims = []\n",
    "\n",
    "    f_stdout = io.StringIO()\n",
    "\n",
    "    for i in indices:\n",
    "        im = read_img(dataset_path + filenames[i], align_input=False)\n",
    "        ims.append(im)\n",
    "\n",
    "        with redirect_stdout(f_stdout):\n",
    "            w_e4e = project_e4e(im, 'pretrained/e4e_ffhq_encode.pt')[1]\n",
    "            empty_img, w_fse, fse_features = project_fse_without_image_generation(\n",
    "                im,\n",
    "                model_path='pretrained/143_enc.pth',\n",
    "                fse_config_name='001',\n",
    "                arcface_model_path='pretrained/backbone.pth',\n",
    "                stylegan_model_path='pretrained/StyleGAN2/stylegan2-ffhq-config-f.pt'\n",
    "            )\n",
    "        ws_ims.append([w_e4e, w_fse, w_fse, w_fse])\n",
    "        fse_features_ims.append([None, None, fse_features, fse_features])\n",
    "        generator_shift_ims.append([False, False, False, True])\n",
    "\n",
    "    print('List of used encoders:')\n",
    "    print('1) E4E latents')\n",
    "    print('2) FSE with latents only')\n",
    "    print('3) FSE with latents and feature map')\n",
    "    print('4) FSE with latents, feature map and generator shift')\n",
    "    \n",
    "    return ims, ws_ims, fse_features_ims, generator_shift_ims, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824d55c-1933-4830-b5bd-f1319b966969",
   "metadata": {},
   "outputs": [],
   "source": [
    "ims, ws_ims, fse_features_ims, generator_shift_ims, random_indices = inverse_images(n_images=24)\n",
    "result = test_inversion_on_different_encoders(\n",
    "    gan_domain='ffhq', s_domain='murasaki_nora_asuya', ims=ims,\n",
    "    w_enc_list=ws_ims,\n",
    "    features_list=fse_features_ims, \n",
    "    generator_shift_list=generator_shift_ims,\n",
    "    image_size=256, verbose=False\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f38e5-15d6-4bc7-8531-3b166e8d27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames[974], filenames[2371], filenames[129], filenames[504], filenames[40], filenames[316], filenames[418], \n",
    "worthful_indices = [974, 2371, 129, 504, 50, 144, 1883, 1597, 316, 418, 2898, 1441, 749, 713, 1505, 360, 2455, 132, 967, 1301, 546, 887, 2402, 2252]\n",
    "worthful_indices += [2372, 2747, 2861, 1695, 114, 2515, 798, 1677, 2089, 728, 2404, 1536, 1733, 1215, 1868, 3003, 2817, 2686]\n",
    "\n",
    "ims, ws_ims, fse_features_ims, generator_shift_ims, _ = inverse_images(\n",
    "    indices=worthful_indices,\n",
    "    use_random_indices=False\n",
    ")\n",
    "\n",
    "result = test_inversion_on_different_encoders(\n",
    "    gan_domain='ffhq', s_domain='jojo', ims=ims,\n",
    "    w_enc_list=ws_ims,\n",
    "    features_list=fse_features_ims, \n",
    "    generator_shift_list=generator_shift_ims,\n",
    "    image_size=1024, verbose=False,\n",
    "    row_names=[filenames[x] for x in worthful_indices],\n",
    "    column_names=[\"orig\", \"e4e latents\", \"fse latents\", \"fse features\", \"fse generator shift\"]\n",
    ")\n",
    "result.save('inversion_comparison_3.pdf', save_all=True, append_images=[])\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-StyleDomain-env]",
   "language": "python",
   "name": "conda-env-.conda-StyleDomain-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
